{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find if model is stationary, if not then include differencing\n",
    "# identify order of model needed (ACF and PACF)\n",
    "# search over models over some model orders to find the best one according to AIC\n",
    "# check over the predictions it makes (diagnostics) before you would move it into production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-machine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load in the time series\n",
    "candy = pd.read_csv('candy_production.csv', \n",
    "            index_col='date',\n",
    "            parse_dates=True)\n",
    "\n",
    "# Plot and show the time series on axis ax\n",
    "fig, ax = plt.subplots()\n",
    "candy.plot(ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# Split the data into a train and test set\n",
    "candy_train = candy.loc[:'2006']\n",
    "candy_test = candy.loc['2007':]\n",
    "\n",
    "# Create an axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the train and test sets on the axis ax\n",
    "candy_train.plot(ax=ax)\n",
    "candy_test.plot(ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for stationary\n",
    "# Import augmented dicky-fuller test function\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Run test\n",
    "result = adfuller(earthquake['earthquakes_per_year'])\n",
    "\n",
    "# Print test statistic\n",
    "print(result[0])\n",
    "\n",
    "# Print p-value\n",
    "print(result[1]) # <0.05 good, reject the null hypothesis that the data is non-stationary\n",
    "\n",
    "# Print critical values\n",
    "print(result[4]) \n",
    "\n",
    "# Calculate the first difference of the time series\n",
    "city_stationary = city.diff().dropna()\n",
    "\n",
    "# Run ADF test on the differenced time series\n",
    "result = adfuller(city_stationary['city_population'])\n",
    "\n",
    "# Plot the differenced time series\n",
    "fig, ax = plt.subplots()\n",
    "city_stationary.plot(ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# Print the test statistic and the p-value\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1]) # calculate the second difference (diff().diff()) if ADF >0.05\n",
    "\n",
    "# Calculate log-return and drop nans\n",
    "amazon_log = np.log(amazon/(amazon.shift(1)))\n",
    "amazon_log = amazon_log.dropna()\n",
    "\n",
    "# Run test and print\n",
    "result_log = adfuller(amazon_log['close'])\n",
    "print(result_log)\n",
    "# both the differenced and the log-return transformed time series have a small p-value, but the log transformed time series has a much more negative test statistic. \n",
    "# This means the log-return tranformation is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-argument",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the generate_arma_data() function must take the minus of the AR parameters for lag greater than zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ARMA model\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "\n",
    "# Instantiate the model\n",
    "model = ARMA(sample['timeseries_1'], order=(2,0))\n",
    "\n",
    "# Fit the model\n",
    "results = model.fit()\n",
    "\n",
    "# Print summary\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-step ahead; always forecasting just one day ahead\n",
    "# Generate predictions\n",
    "one_step_forecast = results.get_prediction(start=-30)\n",
    "\n",
    "# Extract prediction mean\n",
    "mean_forecast = one_step_forecast.predicted_mean\n",
    "\n",
    "# Get confidence intervals of  predictions\n",
    "confidence_intervals = one_step_forecast.conf_int()\n",
    "\n",
    "# Select lower and upper confidence limits\n",
    "lower_limits = confidence_intervals.loc[:,'lower close']\n",
    "upper_limits = confidence_intervals.loc[:,'upper close']\n",
    "\n",
    "# Print best estimate  predictions\n",
    "print(mean_forecast)\n",
    "\n",
    "# plot the amazon data\n",
    "plt.plot(amazon.index, amazon, label='observed')\n",
    "\n",
    "# plot your mean predictions\n",
    "plt.plot(mean_forecast.index, mean_forecast, color='r', label='forecast')\n",
    "\n",
    "# shade the area between your confidence limits\n",
    "plt.fill_between(lower_limits.index, lower_limits, \n",
    "               upper_limits, color='pink')\n",
    "\n",
    "# set labels, legends and show plot\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Amazon Stock Price - Close USD')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic; not just for tomorrow, but for next week or next month; model makes predictions with no corrections, unlike the one-step-ahead prediction\n",
    "# Generate predictions\n",
    "dynamic_forecast = results.get_prediction(start=-30, dynamic=True)\n",
    "\n",
    "# Extract prediction mean\n",
    "mean_forecast = dynamic_forecast.predicted_mean\n",
    "\n",
    "# Get confidence intervals of predictions\n",
    "confidence_intervals = dynamic_forecast.conf_int()\n",
    "\n",
    "# Select lower and upper confidence limits\n",
    "lower_limits = confidence_intervals.loc[:,'lower close']\n",
    "upper_limits = confidence_intervals.loc[:,'upper close']\n",
    "\n",
    "# Print best estimate predictions\n",
    "print(mean_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA: hard way (taking the difference, modeling the difference and then integrating)\n",
    "# Take the first difference of the data\n",
    "amazon_diff = amazon.diff().dropna()\n",
    "\n",
    "# Create ARMA(2,2) model\n",
    "arma = SARIMAX(amazon_diff, order=(2,0,2))\n",
    "\n",
    "# Fit model\n",
    "arma_results = arma.fit()\n",
    "\n",
    "# Print fit summary\n",
    "print(arma_results.summary())\n",
    "\n",
    "# Make arma forecast of next 10 differences\n",
    "arma_diff_forecast = arma_results.get_forecast(steps=10).predicted_mean\n",
    "\n",
    "# Integrate the difference forecast\n",
    "arma_int_forecast = np.cumsum(arma_diff_forecast)\n",
    "\n",
    "# Make absolute value forecast\n",
    "arma_value_forecast = arma_int_forecast + amazon.iloc[-1,0]\n",
    "\n",
    "# Print forecast\n",
    "print(arma_value_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-bandwidth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA: easy way\n",
    "\n",
    "# Create ARIMA(2,1,2) model\n",
    "arima = SARIMAX(amazon, order=(2,1,2))\n",
    "\n",
    "# Fit ARIMA model\n",
    "arima_results = arima.fit()\n",
    "\n",
    "# Make ARIMA forecast of next 10 values\n",
    "arima_value_forecast = arima_results.get_forecast(steps=10).predicted_mean\n",
    "\n",
    "# Print forecast\n",
    "print(arima_value_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing AR or MA model using ACF and PACF\n",
    "\n",
    "# Import\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Create figure\n",
    "fig, (ax1, ax2) = plt.subplots(2,1, figsize=(12,8))\n",
    " \n",
    "# Plot the ACF of df\n",
    "plot_acf(df, lags=10, zero=False, ax=ax1)\n",
    "\n",
    "# Plot the PACF of df\n",
    "plot_pacf(df, lags=10, zero=False, ax=ax2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Instantiate model\n",
    "model = SARIMAX(earthquake, order =(1,0,0)) # AR(1) model\n",
    "model = SARIMAX(earthquake, order =(1,0,0), trend='c') # c for constant trend \n",
    "\n",
    "# Train model\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-private",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching over model order using AIC and BIC\n",
    "\n",
    "# Create empty list to store search results\n",
    "order_aic_bic=[]\n",
    "\n",
    "# Loop over p values from 0-2\n",
    "for p in range(3):\n",
    "  # Loop over q values from 0-2\n",
    "    for q in range(3):\n",
    "      \t# create and fit ARMA(p,q) model\n",
    "        model = SARIMAX(df, order=(p,0, q))\n",
    "        results = model.fit()\n",
    "        \n",
    "        # Append order and results tuple\n",
    "        order_aic_bic.append((p,q,results.aic,results.bic))\n",
    "\n",
    "# Construct DataFrame from order_aic_bic\n",
    "order_df = pd.DataFrame(order_aic_bic, \n",
    "                        columns=['p', 'q', 'AIC', 'BIC'])\n",
    "\n",
    "# Print order_df in order of increasing AIC\n",
    "print(order_df.sort_values('AIC'))\n",
    "\n",
    "# Print order_df in order of increasing BIC\n",
    "print(order_df.sort_values('BIC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower AIC = better model, but sometimes the model fitting procedure fails, so use try/except\n",
    "\n",
    "# Loop over p values from 0-2\n",
    "for p in range(3):\n",
    "    # Loop over q values from 0-2\n",
    "    for q in range(3):\n",
    "      \n",
    "        try:\n",
    "            # create and fit ARMA(p,q) model\n",
    "            model = SARIMAX(earthquake, order=(p,0,q))\n",
    "            results = model.fit()\n",
    "            \n",
    "            # Print order and results\n",
    "            print(p, q, results.aic, results.bic)\n",
    "            \n",
    "        except:\n",
    "            print(p, q, None, None)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model diagnostics\n",
    "\n",
    "# Fit model\n",
    "model = SARIMAX(earthquake, order=(1,0,1))\n",
    "results = model.fit()\n",
    "\n",
    "# Calculate the mean absolute error from residuals\n",
    "mae = np.mean(np.abs(results.resid))\n",
    "\n",
    "# Print mean absolute error\n",
    "print(mae)\n",
    "\n",
    "# Make plot of time series for comparison\n",
    "earthquake.plot()\n",
    "plt.show() # see how the MAE compares to the spread of the time series\n",
    "\n",
    "# Ideal model: residuals should be uncorrelated white Gaussain noise centered on zero\n",
    "\n",
    "# Prob(Q): : p-value for null hypothesis that residuals are uncorrelated (>0.05 good)\n",
    "# Prob(JB): p-value for null hypothesis that residuals are normally distributed (>0.05 good)\n",
    "\n",
    "# Signs for good fit:\n",
    "# Standardized residual: There are no obvious patterns in the residuals\n",
    "# Histogram plus kde estimate: The KDE curve should be very similar to the normal distribution\n",
    "# Normal Q-Q: Most of the data points should lie on the straight line\n",
    "# Correlogram: 95% of correlations for lag greater than one should not be significant\n",
    "\n",
    "# Create and fit model\n",
    "model = SARIMAX(df, order=(1,1,1))\n",
    "results=model.fit()\n",
    "\n",
    "# Create the 4 diagostics plots\n",
    "results.plot_diagnostics()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The JB p-value is zero, which means you should reject the null hypothesis that the residuals are normally distributed. However, the histogram and Q-Q plots show that the residuals look normal. This time the JB value was thrown off by the one outlying point in the time series. In this case, you could go back and apply some transformation to remove this outlier or you probably just continue to the production stage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
